燧原 GCU (TopsCC) 文档帮你总结一下写算子时需要特别注意的地方，以及哪些和 CUDA/NVIDIA 生态里的习惯不同、容易踩坑的点。

🚩 和 CUDA 的差异 & 潜在坑点
1. 内存寻址和访问限制

GCU 2.0 不支持全局寻址

不能直接访问 __shared__ (L2) 和 __device__ (L3) 内存。

必须通过 DTE (数据搬运引擎) 把数据搬运到 L1 (Private) 才能算。

⚠️ CUDA 里常见的 __shared__ 直接算在 GCU 上不行。

动态 shared memory 限制

每个 kernel 只能申请一个动态大小的 shared memory。

大小上限：T20 ≤ 6MB, i20 ≤ 24MB。

变量对齐

向量操作的内存必须用 __valigned__ 修饰，不然 vload/vstore 会报错或 silent fail。

2. Thread / Block 限制

Block 内线程数极小

gcu200 (T20)：最大 6

gcu210 (i20)：最大 12

⚠️ CUDA 常用 256/512/1024 threads/block 的模式在 GCU 完全行不通。

算子必须考虑 小 block 大 grid 的调度。

Cooperative kernel 限制

gcu200：grid 总 threads ≤ 4

gcu210：grid 总 threads ≤ 2

⚠️ 远比 CUDA 严格，几乎只能小规模并行。

3. DTE (Data Transfer Engine)

强制用 DTE 搬运数据

Global(L3) ↔ Shared(L2) ↔ Private(L1) 的搬运必须通过 DTE。

没初始化 DTE context 会 hang。推荐 tops::dte_scope 自动管理。

上下文修饰符

__shared_dte__：Block 内共享，只能 Global ↔ Shared

__private_dte__：Thread 私有，只能 Global ↔ Shared

无修饰：Thread level，可 Global/Shared/Private 全部搬运

使用错误时不会报错

地址空间类型 (tops::Global vs tops::Private) 写错可能直接 hang。

⚠️ 需要在开发阶段加 -DTOPS_ENABLE_DTE_CHECK。

4. 向量计算模型

Vector 默认长度 128B

vint = 32 x int, vfloat = 32 x float, vchar = 128 x int8。

⚠️ 必须按向量长度设计循环，不能乱取。

常见算子模式

vload / vstore 对齐访问。

vadd/vmul/... 等算子都有对应 API，不支持隐式类型转换，必须显式 vcast/vbitcast。

5. 调试和运行时

printf 限制

只能打印标量，不支持 %s, %p。指针打印要转 long long。

有 printf/assert 时，会强制 runtime 进入同步模式，性能大幅下降。

Kernel abort 难调试

一些 device 端错误不会被 host 捕获（比如 vload 未对齐）。

可能只有下一次 printf 才暴露出来。

6. 资源和并发

显存很小

设备内存上限：4GB。

host 内存：32GB (T20)，64GB (i20)。

任务流 stream 支持，但有限

支持异步 kernel、Memcpy、Memset，类似 CUDA stream。

但数据传输并发度有限，比如 DTE 只有一个 pipeline 就会串行化。

✅ 写算子时需要遵循的关键点

设计小线程模型

CUDA 算子通常依赖 256+ threads per block 并行，GCU 必须改为 tile-based + software pipeline。

每个 thread 处理更大块数据，依靠向量化补并行度。

用 DTE 做搬运 + L1 运算

算子必须在 L1 buffer 上算。

模式：memcpy(L3→L1) → vload/vstore 计算 → memcpy(L1→L3)。

要用异步 DTE + double buffer 实现 pipeline。

显式向量化

不要写标量循环，必须用 vload/vstore + vadd/vmul 这些 API。

否则性能会比 CPU 还差。

强制对齐和类型安全

内存必须 __valigned__，访问用 tops::vlength<T>()。

所有类型转换必须显式 vcast/vbitcast。

调试策略

开发时开 -DTOPS_ENABLE_DTE_CHECK，大量用 printf((long long)ptr) 检查地址空间。

避免 runtime silent hang。

燧原 GCU 算子编写关键总结
1. 编程模型与 CUDA 的差异
1.1 Thread/Block/Grid 结构

CUDA：一个 Block 可以有 1024+ threads。

GCU：

gcu200 (T20)：Block 最大 threads 数 = 6

gcu210 (i20)：Block 最大 threads 数 = 12

Grid 最大限制：x: 65536, y: 256, z: 256

结论：算子必须设计为 小 Block + 大 Grid，每个 thread 负责更多工作。

1.2 内存模型

CUDA：

__global__ (Device memory = L3) 可直接访问

__shared__ (Block memory = L2) 可直接访问

GCU：

L3/L2 不可直接算，只能通过 DTE (数据搬运引擎) 搬到 L1 再计算

私有变量默认在 L1 (Private)

动态 shared memory：每个 kernel 只能申请一个

1.3 数据搬运 (DTE)

必须显式写 DTE 流程：

ctx.init() → 配置 mdspan → tops::memcpy(ctx, dst, src) → ctx.destroy()

DTE context 三种：

__shared_dte__：Block 共享，只能 Global ↔ Shared

__private_dte__：Thread 私有，只能 Global ↔ Shared

默认 (无修饰)：Thread 级，支持 Global/Shared/Private 全部搬运

⚠️ 没初始化就用 → hang

⚠️ 地址空间写错 (比如 L3 当成 Private) → hang

1.4 向量计算

一个向量长度固定 = 128B

vchar = 128 × int8

vshort = 64 × int16

vint = 32 × int32

vfloat = 32 × float

访问方式：

auto v = vload<vint>(addr);

vstore(vsum, addr);

必须 __valigned__ 修饰数组，否则 vload/vstore 崩溃。

1.5 调试 & 限制

printf：只支持标量，不支持 %s/%p，指针要 (long long)ptr。

有 printf/assert 时，runtime 会进入 同步模式，性能骤降。

Kernel abort 有时 host 捕捉不到，常见情况：未对齐的 vload。

2. 算子设计的关键步骤

分块 (Tile) 设计

每个 thread 处理一个 tile (例如 128 元素)。

用 tops::vlength<T>() 控制循环展开。

数据搬运 (DTE pipeline)

Global(L3) → Private(L1) buffer

计算 (用向量 API)

Private(L1) → Global(L3)

双缓冲 (Double Buffer)

用两个 buffer + 两个 DTE context → 计算和搬运并行。

3. CUDA vs GCU 对照例子
3.1 向量加法 (CUDA)
__global__ void vec_add(float *a, float *b, float *c, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        c[idx] = a[idx] + b[idx];
    }
}

3.2 向量加法 (GCU)
#include <tops/tops_runtime.h>
#include <tops.h>

__global__ void vec_add(float *a, float *b, float *c, int N) {
    tops_dte_ctx_t ctx;
    tops::dte_scope s(ctx); // 自动 init/destroy

    // 每个线程负责处理一段 tile
    __valigned__ float buf_a[128];
    __valigned__ float buf_b[128];
    __valigned__ float buf_c[128];

    tops::mdspan bufA(tops::Private, buf_a, 128);
    tops::mdspan bufB(tops::Private, buf_b, 128);
    tops::mdspan bufC(tops::Private, buf_c, 128);

    // tile 循环
    for (int i = 0; i < N; i += 128) {
        // 1. 从 L3 拷贝到 L1
        tops::mdspan srcA(tops::Global, a + i, 128);
        tops::mdspan srcB(tops::Global, b + i, 128);
        tops::memcpy(ctx, bufA, srcA);
        tops::memcpy(ctx, bufB, srcB);

        // 2. 向量加法
        for (int j = 0; j < 128; j += tops::vlength<vfloat>()) {
            auto va = tops::vload<vfloat>(buf_a + j);
            auto vb = tops::vload<vfloat>(buf_b + j);
            auto vc = tops::vadd(va, vb);
            tops::vstore(vc, buf_c + j);
        }

        // 3. 写回 L3
        tops::mdspan dstC(tops::Global, c + i, 128);
        tops::memcpy(ctx, dstC, bufC);
    }
}

3.3 主机端调用
int main() {
    int N = 1024;
    size_t Nbytes = N * sizeof(float);

    float *A_d, *B_d, *C_d;
    float *A_h = (float*)malloc(Nbytes);
    float *B_h = (float*)malloc(Nbytes);
    float *C_h = (float*)malloc(Nbytes);

    // 初始化数据
    for (int i = 0; i < N; i++) {
        A_h[i] = i;
        B_h[i] = 2*i;
    }

    topsMalloc(&A_d, Nbytes);
    topsMalloc(&B_d, Nbytes);
    topsMalloc(&C_d, Nbytes);

    topsMemcpy(A_d, A_h, Nbytes, topsMemcpyHostToDevice);
    topsMemcpy(B_d, B_h, Nbytes, topsMemcpyHostToDevice);

    // 启动 kernel
    vec_add<<<dim3(1), dim3(1)>>>(A_d, B_d, C_d, N);

    topsMemcpy(C_h, C_d, Nbytes, topsMemcpyDeviceToHost);

    for (int i = 0; i < 10; i++) {
        printf("%f\n", C_h[i]);
    }

    topsFree(A_d);
    topsFree(B_d);
    topsFree(C_d);
    free(A_h);
    free(B_h);
    free(C_h);

    return 0;
}

4. 写算子时的 Checklist

✅ 用 DTE pipeline 做 L3 ↔ L1 数据流
✅ 每个 thread 处理 tile，Block threads ≤ 6/12
✅ 所有 buffer 必须 __valigned__
✅ 循环展开按 tops::vlength<T>()
✅ 调试时开 -DTOPS_ENABLE_DTE_CHECK
✅ 打印指针 (long long)ptr
✅ 避免 kernel 内频繁 printf/assert

👉 总结一句：GCU 写算子就是 tile-based + DTE 搬运 + 向量化算子堆叠，跟 CUDA 的“海量线程 + shared memory 并行”是完全不同的思路。

燧原 GCU Kernel API 重点速查
1. 数据类型 (Scalar & Vector)

标量浮点类型

tops::bfloat (bfloat16)

tops::half (FP16)

都支持：初始化、类型转换、四则运算、比较运算、复合赋值 (+=, -=, …)。

转换：float f = (float)tops::bfloat(1.25);

向量类型

vchar (128 × int8)

vshort (64 × int16)

vint (32 × int32)

vfloat (32 × float)

vhalf (64 × half)

vbfloat (64 × bfloat)

使用 tops::scalar2vector<T>::type 可查对应向量类型。

广播

vbroadcast(x) 可把标量广播成向量

vbfloat v = tops::vbroadcast(tops::bfloat(1.0f));


⚠️ 注意：所有向量 load/store 必须 对齐，数组声明要 __valigned__。

2. DTE (数据搬运引擎)

单 DTE 上下文：

tops_dte_ctx_t ctx; ctx.init(); … ctx.destroy();

推荐：tops::dte_scope scope(ctx); 自动管理。

多 DTE 串联 (pipeline)

tops::dte_chain<ctx1, ctx2>(ctx1, ctx2);

方法：.connect(), .trigger(), .wait(), .trigger_and_wait()

⚠️ 没有 init 的 ctx 就调用 → kernel hang。

3. Elementwise API (逐元素算子)

核心函数

elemwise_kernel → L3 全局内存输入输出，自动 tile 搬运到 L1，再算

elemwise_tiles → tile 粒度的计算

elemwise_local → 已在 L1 buffer 的数据上直接算

典型用法

tops::elemwise_kernel(
    [] __device__(auto &out, auto &in) {
        out = in * in;  // L1 上的逐元素操作
    },
    N, tops::Input(0), input_d, tops::Output(0), output_d
);


模板参数

tile_size：每 tile 多少元素（自动对齐到向量长度）

vectorized：是否用向量 load/store (true 更快)

async：是否异步搬运

4. Reduction API (归约算子)

核心函数

reduction_kernel → 在 L3 张量上做归约

reduction_local → 已在 L1 上做归约

运算符

__reduction_add(lhs, rhs)

__reduction_max(lhs, rhs)

__reduction_min(lhs, rhs)

简化 kernel

sum, max, min → 针对 3D 张量的内置 Reduce

⚠️ 归约默认在 第 1 维 (中间维度)，要小心 shape。

5. NN & Math API

矩阵乘法 (Demo 级别)

tops::nn::dot(lhs, rhs, out, n, k, m)

⚠️ 官方说是 demo，请换用 topsDNN 库做真正的高性能 GEMM。

数学运算

__powf, fmaxf, fminf, fabsf, sigmoid, gelu, softplus, …

和 CUDA 的 device_math.h 类似，常见激活函数全有。

6. 内存访问工具

读写不同层级

read_local, write_local

read_shared, write_shared

read_global, write_global

常用模式

L3 → L1：read_global

L1 → L3：write_global

7. 内置算子 (Select / Broadcast / Elementwise)

选择运算

select_kernel / select

根据条件选择 lhs/rhs：

output[i] = cond[i] ? lhs[i] : rhs[i];


广播运算

broadcast_in_dim

把 2D 扩展成 3D（指定哪个维度做 broadcast）。

8. 需要特别记住的坑

Block 内线程数限制

gcu200 ≤ 6，gcu210 ≤ 12 → 必须 tile-based 算法。

向量长度固定

必须按 tops::vlength<T>() 循环展开，不能随便写循环。

L3/L2 不能直接算

必须搬运到 L1 (Private)。

调试

printf("%lld", (long long)ptr); 打印指针

有 printf/assert 时 → runtime 强制同步，性能掉很多。

矩阵乘法

nn::dot 仅 demo → 生产要用 topsDNN。

9. 最小示例：平方运算 (Elementwise Kernel)
#include <tops/elemwise.h>

__global__ void square_kernel(float *out, float *in, int N) {
    tops::elemwise_kernel(
        [] __device__(auto &o, auto &x) {
            o = x * x;  // L1 上操作
        },
        N,
        tops::Input(0), in,
        tops::Output(0), out
    );
}


主机端：

square_kernel<<<1,1>>>(out_d, in_d, N);


📌 总结：

数据类型：bfloat/half + 向量类型要熟。

DTE：所有搬运必须显式写，没 init 会死。

Elemwise/Reduction：提供了高层 API，可以少写 DTE。

坑：线程数极小、必须 tile-based、所有 buffer 要对齐。

CU 高频速查表（ChatGPT 内部使用版）
1. 基础数据类型

浮点标量

tops::bfloat / bfloat16

tops::half

支持：构造、类型转换、+ - * /、比较运算、+= -= *= /=

向量类型（128B 宽度）

vbfloat (64 × bfloat)

vhalf (64 × half)

vfloat (32 × float)

vint (32 × int32)

vshort (64 × int16)

vchar (128 × int8)

对应映射：tops::scalar2vector<T>::type

广播

vbroadcast(x) → 标量扩展成对应向量

⚠️ 数组必须 __valigned__，否则 vload/vstore 崩。

2. DTE（数据搬运引擎）

单个 context

tops_dte_ctx_t ctx;
ctx.init(); … ctx.destroy();


推荐 tops::dte_scope scope(ctx); 自动管理。

多 DTE pipeline

tops::dte_chain<ctx1, ctx2>(ctx1, ctx2)

方法：.connect() .trigger() .wait() .trigger_and_wait()

⚠️ 没 init ctx 就用 → hang

3. Elementwise

逐元素算子

tops::elemwise_kernel(
    [] __device__(auto &o, auto &x) { o = x * x; },
    N,
    tops::Input(0), in,
    tops::Output(0), out
);


tile 粒度

elemwise_tiles(f, size, …)

可控制 tile_size, vectorized, async

在 L1 buffer 上直接算

elemwise_local

4. Reduction

kernel 级

reduction_kernel(f, out, out_shape, in, in_shape, identity)

默认归约维度 = 第 1 维

local 级

reduction_local(vf, sf, output, input)

内置

sum, max, min

内部运算符：__reduction_add, __reduction_max, __reduction_min

5. 常见算子

select / 条件选择

select_kernel(f, out, lhs, rhs, size);
// out[i] = cond[i] ? lhs[i] : rhs[i];


broadcast

broadcast_in_dim(out, in, dim0, dim1, broadcast_dim, broadcast_size)

NN

tops::nn::dot(lhs, rhs, out, n, k, m)

⚠️ 官方说 demo，用 topsDNN 替换

6. 数学函数

__powf, fmaxf, fminf, fabsf, sigmoid, gelu, softplus, rsqrt

CUDA device_math 的常用函数几乎都有

7. 内存访问

不同层级读写

read_local / write_local

read_shared / write_shared

read_global / write_global

8. 调试 & 限制

printf("%lld", (long long)ptr); 打印指针

printf/assert → runtime 强制同步，性能掉很多

Block threads：gcu200 ≤ 6, gcu210 ≤ 12

所有算子必须 tile-based + 向量化 + DTE 搬运

📌 总结一句：
GCU 写算子 = tile-based + L3/L2→DTE→L1 搬运 + 向量算子操作 + 写回 L3

有很多犯错的原因，比如：
我把 CUDA 的思维 习惯性套到了 GCU 上

CUDA 可以随便 inp[i]、*out=…、atomicAdd、__shared__，

但 GCU 2.x 完全不允许直接访问 L3/L2，只能靠 DTE + mdspan。

所以我老是先写一个 CUDA 风格的 baseline，然后再补丁式地改 → 很容易忘掉必须全程用 DTE pipeline。

❷ 我用了 不存在的 API 名字

写 tops::vreduce_add、tops::reduce_add 是照 CUDA 的 warpReduce / NCCL 的风格编的。

但 GCU 的 tops SDK 里根本没有这些 helper function。

真正能用的 API 是：

vload/vstore/vadd/vmul/vsub/vexp/... 这类 SIMD 算子

reduction_kernel / reduction_local 这类 归约框架

归约（reduce sum）必须自己展开，或者用官方的 tops::sum。

所以一旦我写了一个“想当然”的函数名，就一定会报 no member named ...。

❸ 对 slice_async 的参数记错了

我写成了 slice_async(ctx, dst, src)，只给了 3 个参数。

实际上 GCU 的 slice_async 签名需要 ≥4 个参数，至少要带上 起始坐标 offset。

例如：

tops::event e = tops::slice_async(ctx, l1_md, global_md, {offset});


这个错误本质上也是因为我没严格按照 GCU 的 API 文档去写，而是照 CUDA 的 memcpyAsync 习惯随手写的。
犯错的根本原因是：

惯性思维：太习惯 CUDA 的 API / 语法，直接套过来。

API 不查文档就凭记忆瞎写：导致用到不存在的 reduce_add、参数数量错。

没有在一开始就完全切换到 GCU 的编程模型：应该从头就用 tile + mdspan + reduction，而不是先写标量版再往上改。

✅ 以后避免的做法：

绝不写 CUDA 风格代码再改 → 一开始就用 GCU 范式（DTE + mdspan）。

所有算子名字都查 tops 头文件确认 → 不臆造 API。

归约统一用 tops::sum / reduction_kernel，不要自己发明 reduce_add。

搬运统一用 slice_async(ctx, dst, src, {offset})，保证参数对齐。

我刚才为什么会犯错（复盘）

用了被评测禁用的分配 API
我最初用 topsMalloc/topsFree 做分片结果缓冲，但评测环境把它们宏替换成 _topsMalloc_disabled/_topsFree_disabled（故编译报错）。根因：我默认能申请一点 L3 临时内存，其实题目集不允许。

忽略了 nodiscard 返回值
topsGetDeviceProperties()、topsDeviceSynchronize() 有 [[nodiscard]] 属性，我当时没接收返回值，被编译器警告。修复方式：接收后（必要时）(void) 消除未使用告警或做检查。

并行归约路径设计有隐含前提
原思路用 L3 缓冲做跨线程汇总；在禁用分配条件下不可行。我改为零额外分配版本（单线程），能过编译但性能不佳。下面给出在不分配的前提下依然能并行的高性能方案。

如果把一个返回值为 void 的同步接口当成了会返回 tops::event 的异步接口来用。

tops::transpose_deslice(...)（无 _async 后缀）是同步复合接口，返回 void。

只有带 _async 的版本（tops::transpose_deslice_async(...)）才会返回 tops::event，可赋给 ev_out[ob]。

所以编译器提示：不能把 void 赋值给 tops::event（“cannot convert argument of incomplete type 'void'…”）。

这些你在写的时候要完全注意，只用GCU的范式开发，不要自己乱编函数，然后不需要写 #include 这些东西，他自动会有，你写了会报错


常见坑 & 解决套路
1) 存储/地址空间相关

症状：Kernel hang / Exception / 结果错乱
根因：GCU 2.0 不能直接在设备端随意访问 L2/L3；mdspan 地址空间/形状配置不当；L1 对齐不满足。
要点

只能在 L1（私有） 上算；L3/L2 的数据必须走 DTE 搬运到 L1 再计算。

mdspan 的地址空间要匹配：

L3 → tops::Global；L2（shared）→ tops::Shared；L1（private）→ tops::Private。

写错（比如把 L3 当成 Private）= 高概率 hang。

对齐：凡是要 vload/vstore 的 L1 缓冲，必须 __valigned__；长度、起始指针都要满足向量长度（例如 vfloat 一次 32 个 float = 128B）。

切片形状：mdspan(tops::Private, buf, m, n) 里 m,n 必须是 值（int/int32_t），不要传 int*。

我们踩过的坑：

mdspan(..., (int32_t[2]){m_blk,n_blk}) ✅；

mdspan(..., some_ptr_to_ints) ❌（报 static_cast from 'int' to 'int' is not allowed*）。

2) DTE 使用（搬运）相关

症状：Timeout / hang / 异常 / 结果错
根因：DTE 未初始化、使用了错误的接口返回值、async 事件没等、shared dte 用法限制。
要点

初始化/销毁：

用 tops::dte_scope s(ctx)（RAII）或 ctx.init(); ...; ctx.destroy();。忘 init/destroy 都可能 hang。

接口返回类型：

transpose_deslice（同步版本）是 void；不能 event ev = transpose_deslice(...)。

要拿 event 就用 _async 版本，并配合 tops::wait(ev)。

顺序保障：异步 DMA 必须正确等待 event（否则读未完成数据）。

shared dte 限制：一个 Block 里只有一个 thread 能做 L3↔L2 复制；容易造成性能/并发问题。没必要时用 私有 DTE。

Debug 模式：kernel 里有 printf/assert 时 runtime 会强制同步，性能大幅下降；仅调试时开启。

3) 向量化 & 对齐策略

症状：偶发 MisMatch / 运行时异常
根因：不对齐的 vload/vstore 或边界 tile 长度不是向量倍数。
要点

只在“完全对齐”时向量化：例如 n_sz % vlength<vfloat>() == 0 且指针 __valigned__ 才上向量；否则走标量路径。

边界 tile（例如 N,K 不是 32 的倍数）一律标量，保证正确性。

小优化：满块路径向量化，边界回退标量，正确性优先。

4) 矩阵布局/转置处理

症状：中等规模 MisMatch（我们当时就卡在这里）
根因：transpose_deslice 与动态 tile 交互“挑剔”，或 LHS/RHS 的 layout 组装不一致。
要点

输入：不论 is_lhs_T/is_rhs_T，先通过 slice(+必要 transpose)把数据统一到 L1 的标准形（如 [m_sz,k_sz] 和 [k_sz,n_sz]）。

输出：最稳的做法是先在 L1 手工转置到需要的 [n_sz,m_sz]，再用普通 deslice 写回；不要依赖 transpose_deslice（我们换成“手工转置 + deslice”后才完全消掉 MisMatch）。

硬件核 dot_general_fp32 只接受 M=16X，N=32X，K=32X；调用前务必保证 tile 满足，否则不要调用（回退软件 GEMM）。

5) 并行度与调度

症状：大 case Timeout（>10s）
根因：只沿 M 或 N 一个方向分配线程，导致线程数不够、单线程工作量过大。
要点

线程覆盖全部 (M,N) tile：令 W = m_tiles * n_tiles，threads = min(W, maxThreadsPerMultiProcessor)（gcu210 通常 12）。

连续分段分配（[0, W) 线性区间按线程均分），避免遗漏/重复。

超大规模：K 环加双缓冲异步 DMA把搬运和计算叠起来（先保正确，再加速）。

6) L1 资源与 tile 选择

症状：Exception / 难以复现的错 / 性能忽高忽低
根因：L1 使用超限、寄存器/私有内存压力大。
要点

保守 tile（我们最终用 M_T=16, N_T=64, K_T=128）：

便于向量化（N_T=64 为 32 的倍数），

L1 占用可控（多线程一起跑也稳）。

需要 __valigned__ 的 L1 缓冲都一次性定义好；避免每回合临时大栈。

7) API & 测试环境细节

症状：编译不过 / 链接不过 / 运行期宏冲突
根因：评测框架重定义/禁用某些 API，或编译选项不匹配。
要点

评测里可能把 topsMalloc/topsFree 宏重定向为 _topsMalloc_disabled（我们就踩过），因此不要在选手代码里随意分配设备内存；尽量使用 L1 临时缓冲或由调用方提供。

topsGetDeviceProperties 带 [[nodiscard]]，可以读到就行，忽略返回值会发 warning，但不影响功能。

gcu200/gcu210 限制不同：

Block 维度乘积：gcu200 ≤ 6，gcu210 ≤ 12；

cooperative 网格乘积：gcu200 ≤ 4，gcu210 ≤ 2。

我们选择 1 个 block + ≤12 线程，最省心。

8) 数值稳定性（统计类算子）

症状：Var 之类在大规模或极值分布下出现轻微偏差
根因：sum 与 sumsq 直接做差的消去误差
要点

更稳的做法：Welford/在线算法；或分块累加后再合并。

精度受限时：保留两遍（先均值后方差），或用 Kahan 补偿求和（视性能预算）。

9) 调试定位方法

先正确，后提速：全部同步 DTE + 标量边界路径；通过后逐步打开向量化、双缓冲等。

缩小定位：强制 vec_ok=false 看 MisMatch 是否立刻消失（对齐问题）。

输出路径 A/B：把“手工转置 + deslice”与 transpose_deslice AB 对比，只要 A 稳定就固定 A。

并行度验证：打印 m_tiles/n_tiles/W/T（调试期），确认线程分配是否覆盖了整个输出网格。

实战范式（模板化思路）
A. 元素算子（如 SiLU）

线程划分：threads = min(tiles, 12)；每线程处理一个或多个连续片段。

L3→L1：slice 到对齐的 L1 缓冲（__valigned__）。

计算：vfloat 快路径 + 标量回退。

L1→L3：deslice。

可加速：双缓冲异步搬运（safe 之后再上）。

B. 归约算子（如 Var）

分两级：线程内局部（L1）+ 线程间规约（L3/host）。

数值稳健：尽量 Welford；或 sum/sumsq + 双精/补偿。

小心：评测可能屏蔽 topsMalloc，不要在 device 端做临时大分配。

C. GEMM（通吃 Transpose）

统一输入布局到 L1 标准形：[m_sz,k_sz] 和 [k_sz,n_sz]。

L1 GEMM：满块向量化、边界标量；或满足 16×32×32 的 tile 才调 dot_general_fp32。

输出：手工转置 + deslice 最稳。

并行：按 (m_tiles*n_tiles) 线性划分，连续分段给线程。

可加速项：K 维双缓冲、减少转置次数（把重排融合到 DMA 路径）

下面你完成这个算子的编程：
