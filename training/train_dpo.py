# train_dpo_custom.py - æ”¯æŒè‡ªå®šä¹‰JSONæ•°æ®é›†çš„DPOè®­ç»ƒï¼Œä½¿ç”¨SwanLabè®°å½•
import os
import json
import torch
import swanlab
import argparse
from datasets import Dataset, load_dataset
from trl import DPOConfig, DPOTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainerCallback
import time

def load_json_dataset(json_file_path: str, split_ratio: float = 0.9):
    """
    ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†
    
    Args:
        json_file_path: JSONæ–‡ä»¶è·¯å¾„
        split_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼‰
    
    Returns:
        train_dataset, eval_dataset
    """
    print(f"æ­£åœ¨åŠ è½½JSONæ•°æ®é›†: {json_file_path}")
    
    # è¯»å–JSONæ–‡ä»¶
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    if isinstance(data, dict):
        data = [data]
    
    print(f"æ•°æ®æ€»æ•°: {len(data)}")
    
    # éªŒè¯æ•°æ®æ ¼å¼
    required_fields = ['prompt', 'chosen', 'rejected']
    valid_data = []
    
    for i, item in enumerate(data):
        if all(field in item for field in required_fields):
            valid_data.append(item)
        else:
            print(f"è­¦å‘Š: ç¬¬{i+1}æ¡æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ {required_fields}")
    
    print(f"æœ‰æ•ˆæ•°æ®: {len(valid_data)}")
    
    if not valid_data:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®ï¼è¯·ç¡®ä¿æ•°æ®åŒ…å« 'prompt', 'chosen', 'rejected' å­—æ®µ")
    
    # åˆ›å»ºDatasetå¯¹è±¡
    dataset = Dataset.from_list(valid_data)
    
    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    print("æ•°æ®é›†åˆ—å:", dataset.column_names)
    print("æ•°æ®é›†ç‰¹å¾:", dataset.features)
    print("\nç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹:")
    first_example = dataset[0]
    for key, value in first_example.items():
        print(f"{key}: {value}")
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    if len(dataset) > 1:
        split_point = int(len(dataset) * 1)
        train_dataset = dataset.select(range(split_point))
        # eval_dataset = dataset.select(range(split_point, len(dataset)))
        eval_dataset = train_dataset
        print(f"\nè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
        return train_dataset, eval_dataset
    else:
        print(f"\nè®­ç»ƒé›†å¤§å°: {len(dataset)}")
        print("æ•°æ®é‡å¤ªå°‘ï¼Œä¸åˆ†å‰²éªŒè¯é›†")
        return dataset, None

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="DPO training for molecular design")
    parser.add_argument("--train_data_path", required=True, help="Path to the training JSON data")
    parser.add_argument("--output_dir", required=True, help="Output directory for the trained model")
    parser.add_argument("--exp_name", required=True, help="Experiment name")
    parser.add_argument("--model_name_or_path", default="/home/lzz/models/Qwen/Qwen2.5-7B-Instruct", 
                       help="Base model path or previous trained model path")
    parser.add_argument("--ref_model_path", default="/home/lzz/models/Qwen/Qwen2.5-7B-Instruct",
                       help="Reference model path (should always be the original base model)")
    parser.add_argument("--num_train_epochs", type=int, default=1, help="Number of training epochs")
    parser.add_argument("--learning_rate", type=float, default=1e-6, help="Learning rate")
    # ä¸ºäº†é¿å…æ˜¾å­˜ä¸è¶³ï¼Œé»˜è®¤ batch size è°ƒå°ï¼›å¦‚éœ€æ›´å¤§å¯åœ¨å‘½ä»¤è¡Œæ‰‹åŠ¨è¦†ç›–
    parser.add_argument("--per_device_train_batch_size", type=int, default=2, help="Training batch size")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="Gradient accumulation steps")
    parser.add_argument("--beta", type=float, default=0.1, help="DPO beta parameter")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–SwanLab
    swanlab.init(
        project="DPO-MOLLM-Training",
        experiment_name=args.exp_name,
        description="DPOè®­ç»ƒåˆ†å­è®¾è®¡ä»»åŠ¡ï¼Œä½¿ç”¨MOLLMæ•°æ®é›†",
        config={
            "model": args.model_name_or_path,
            "dataset": args.train_data_path,
            "task": "molecule_design_preference_optimization",
            "num_train_epochs": args.num_train_epochs,
            "learning_rate": args.learning_rate,
            "per_device_train_batch_size": args.per_device_train_batch_size,
            "gradient_accumulation_steps": args.gradient_accumulation_steps,
            "beta": args.beta
        }
    )
    
    # è®­ç»ƒæ—¥å¿—è¾“å‡ºåˆ° JSONLï¼ˆæ¯æ­¥ä¸€æ¡ï¼‰
    train_log_dir = "/home/lzz/mollm_results/exp/gemini-2.5-flash-nothinking/train_log"
    os.makedirs(train_log_dir, exist_ok=True)
    step_log_file = os.path.join(train_log_dir, f"{args.exp_name}.jsonl")

    class StepJSONLogger(TrainerCallback):
        def __init__(self, log_path: str, exp_name: str):
            self.log_path = log_path
            self.exp_name = exp_name
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.log_path), exist_ok=True)

        def on_log(self, args, state, control, logs=None, **kwargs):
            if not logs:
                return
            record = {
                "exp_name": self.exp_name,
                "step": int(state.global_step),
                "epoch": float(state.epoch) if state.epoch is not None else None,
                "timestamp": time.time(),
                "logs": logs,
            }
            try:
                with open(self.log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
            except Exception as e:
                print(f"å†™å…¥stepæ—¥å¿—å¤±è´¥: {e}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # æ¸…ç† GPU ç¼“å­˜
    torch.cuda.empty_cache()

    # é…ç½®å‚æ•°
    model_name = args.model_name_or_path
    json_data_path = args.train_data_path
    output_dir = args.output_dir

    # åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ï¼‰
    print(f"æ­£åœ¨åŠ è½½ç­–ç•¥æ¨¡å‹: {model_name}")
    # æ˜¾å¼æŒ‡å®šè®¾å¤‡åˆ†é…ï¼šç­–ç•¥æ¨¡å‹æ”¾åœ¨ GPU 0-3
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="balanced",  # ä½¿ç”¨ balanced ç­–ç•¥åˆ†å¸ƒåœ¨å¤šä¸ª GPU
        max_memory={0: "14GiB", 1: "14GiB", 2: "14GiB", 3: "14GiB", 4: "14GiB", 5: "14GiB", 6: "14GiB", 7: "14GiB"},
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )

    # åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆå§‹ç»ˆæ˜¯åŸå§‹åŸºç¡€æ¨¡å‹ï¼‰
    ref_model_name = args.ref_model_path
    print(f"æ­£åœ¨åŠ è½½å‚è€ƒæ¨¡å‹: {ref_model_name}")
    # æ˜¾å¼æŒ‡å®šè®¾å¤‡åˆ†é…ï¼šå‚è€ƒæ¨¡å‹æ”¾åœ¨ GPU 4-7
    ref_model = AutoModelForCausalLM.from_pretrained(
        ref_model_name,
        torch_dtype=torch.bfloat16,
        device_map="balanced",  # ä½¿ç”¨ balanced ç­–ç•¥åˆ†å¸ƒåœ¨å¤šä¸ª GPU
        max_memory={0: "14GiB", 1: "14GiB", 2: "14GiB", 3: "14GiB", 4: "14GiB", 5: "14GiB", 6: "14GiB", 7: "14GiB"},
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # å¦‚æœ tokenizer æ²¡æœ‰ pad_tokenï¼Œæ·»åŠ ä¸€ä¸ª
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æ•°æ®é›†
    try:
        # é¦–å…ˆå°è¯•åŠ è½½JSONæ•°æ®é›†
        if os.path.exists(json_data_path):
            train_dataset, eval_dataset = load_json_dataset(json_data_path)
        else:
            print(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_data_path}")
            print("ä½¿ç”¨é»˜è®¤çš„arrowæ•°æ®é›†")
            # å›é€€åˆ°åŸæ¥çš„arrowæ•°æ®é›†
            train_dataset = load_dataset(
                'arrow',
                data_files='/home/lzz/verl_1/dataset/train/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555/ultrafeedback_binarized-train.arrow',
                split='train'
            )
            eval_dataset = None
            
    except Exception as e:
        print(f"åŠ è½½JSONæ•°æ®é›†å¤±è´¥: {e}")
        print("ä½¿ç”¨é»˜è®¤çš„arrowæ•°æ®é›†")
        train_dataset = load_dataset(
            'arrow',
            data_files='/home/lzz/verl_1/dataset/train/trl-lib___ultrafeedback_binarized/default/0.0.0/47124cb5778f5d50de1c7676a412828f3ea7c555/ultrafeedback_binarized-train.arrow',
            split='train'
        )
        eval_dataset = None

    # é…ç½®è®­ç»ƒå‚æ•° - ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°é…ç½®
    training_args = DPOConfig(
        output_dir=output_dir,
        
        # æ‰¹æ¬¡å’Œæ¢¯åº¦è®¾ç½®
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        
        # è®­ç»ƒæ­¥æ•°å’Œè°ƒåº¦
        num_train_epochs=args.num_train_epochs,
        
        # ç²¾åº¦å’Œå†…å­˜ä¼˜åŒ–
        bf16=True,                           # ä½¿ç”¨bf16ç²¾åº¦ï¼ˆç›¸å¯¹fp32æ˜¾å­˜å ç”¨æ›´å°ï¼‰
        dataloader_pin_memory=False,         # ä¸ä½¿ç”¨å†…å­˜å›ºå®š
        remove_unused_columns=False,         # ä¿ç•™æ‰€æœ‰åˆ—
        gradient_checkpointing=True,         # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        
        # ä¿å­˜å’Œæ—¥å¿—
        save_steps=50,                       # ä¿å­˜é—´éš”
        logging_steps=1,                     # æ›´é¢‘ç¹çš„æ—¥å¿—è®°å½•
        
        # å­¦ä¹ ç‡ä¼˜åŒ– - å…³é”®è°ƒæ•´
        # warmup_ratio=0.1,                    # ä½¿ç”¨warmupæ¯”ä¾‹è€Œéå›ºå®šæ­¥æ•°
        warmup_steps=0,                      # ä¸ä½¿ç”¨warmup
        lr_scheduler_type="constant",        # ä½¿ç”¨å¸¸æ•°å­¦ä¹ ç‡
        learning_rate=args.learning_rate,    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„å­¦ä¹ ç‡
        
        # DPOç‰¹å®šå‚æ•°ä¼˜åŒ–
        beta=args.beta,                     # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„betaå€¼
        loss_type="sigmoid",                # ä½¿ç”¨æ ‡å‡†sigmoid loss
        label_smoothing=0.1,                # æ·»åŠ æ ‡ç­¾å¹³æ»‘ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
        
        # åºåˆ—é•¿åº¦ï¼ˆå‡å°ä»¥é™ä½æ˜¾å­˜å ç”¨ï¼‰
        max_length=2048,
        max_prompt_length=1536,
        
        # æŠ¥å‘Šå’Œç›‘æ§
        report_to=["swanlab"],               # ä½¿ç”¨SwanLabè®°å½•
        dataloader_num_workers=0,            # å•è¿›ç¨‹æ•°æ®åŠ è½½
        
        # è¯„ä¼°ç­–ç•¥
        eval_strategy="steps" if eval_dataset else "no",
        eval_steps=500 if eval_dataset else None,  # æ›´é¢‘ç¹çš„è¯„ä¼°
        
        # ä¿å­˜ç­–ç•¥
        save_strategy="steps",               # æŒ‰æ­¥æ•°ä¿å­˜
        logging_strategy="steps",            # æŒ‰æ­¥æ•°è®°å½•æ—¥å¿—
        
        # æ¢¯åº¦è£å‰ª
        max_grad_norm=1.0,                   # æ¢¯åº¦è£å‰ªé˜ˆå€¼è°ƒä¸º1.0
        
        # å…¶ä»–ä¼˜åŒ–
        weight_decay=0.01,                   # æ·»åŠ æƒé‡è¡°å‡
        adam_epsilon=1e-6,                   # è°ƒæ•´Adam epsilon
        
        # æ•°æ®å¤„ç†ä¼˜åŒ–
        dataloader_drop_last=True,           # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
    )

    # åˆ›å»ºè®­ç»ƒå™¨ï¼Œæ˜ç¡®æŒ‡å®šå‚è€ƒæ¨¡å‹
    step_json_logger = StepJSONLogger(step_log_file, args.exp_name)
    trainer = DPOTrainer(
        model=model, 
        ref_model=ref_model,  # æ˜ç¡®æŒ‡å®šå‚è€ƒæ¨¡å‹
        args=training_args, 
        processing_class=tokenizer, 
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        callbacks=[step_json_logger]
    )

    # åœ¨è®­ç»ƒå‰å†æ¬¡æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()

    print("å¼€å§‹è®­ç»ƒ...")
    trainer.train()

    print("è®­ç»ƒå®Œæˆï¼")
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model()
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # è®°å½•æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡åˆ°SwanLab
    final_metrics = trainer.state.log_history[-1] if trainer.state.log_history else {}
    if final_metrics:
        swanlab.log({
            "final_train_loss": final_metrics.get("train_loss", 0),
            "final_rewards_accuracy": final_metrics.get("rewards/accuracies", 0),
            "final_rewards_margin": final_metrics.get("rewards/margins", 0),
        })
    
    # ç»“æŸSwanLabè®°å½•
    swanlab.finish()
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼Œæ—¥å¿—å·²è®°å½•åˆ°SwanLabï¼")

if __name__ == "__main__":
    main()
